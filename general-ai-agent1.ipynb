{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12969895,"sourceType":"datasetVersion","datasetId":8208803}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain langchain-groq langgraph llama-index chromadb faiss-cpu\n!pip install langchain_community\n!pip install -q duckduckgo-search\n!pip install -U ddgs\n!pip install -q chromadb\n!pip install langchain_chroma\n!pip install langchain_openai\n!pip install -q sentence-transformers chromadb\n!pip install -q gradio\n\nfrom kaggle_secrets import UserSecretsClient\nimport os\n# load the Groq API key from Kaggle Secrets\nsecrets = UserSecretsClient()\nos.environ[\"GROQ_API_KEY\"] = secrets.get_secret(\"GROQ_API_KEY\")\nprint(\"Groq API key loaded ✅\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:03.482618Z","iopub.execute_input":"2025-09-14T06:03:03.482900Z","iopub.status.idle":"2025-09-14T06:03:32.700488Z","shell.execute_reply.started":"2025-09-14T06:03:03.482865Z","shell.execute_reply":"2025-09-14T06:03:32.699729Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.29)\nRequirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.76)\nRequirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\nRequirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.5)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.13)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\nRequirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.14.0)\nRequirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.32.5->langchain_community) (2025.6.15)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: ddgs in /usr/local/lib/python3.11/dist-packages (9.5.5)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from ddgs) (8.2.1)\nRequirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (0.15.0)\nRequirement already satisfied: lxml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (6.0.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.11/dist-packages (0.2.6)\nRequirement already satisfied: langchain-core>=0.3.76 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.76)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (1.26.4)\nRequirement already satisfied: chromadb>=1.0.20 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (1.0.21)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.2.2.post1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (2.11.7)\nRequirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.4.2)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.34.3)\nRequirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (5.4.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.14.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.22.1)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.37.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.37.0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.37.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.21.2)\nRequirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (1.73.1)\nRequirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.3.0)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.16.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (33.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (8.5.0)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (6.0.2)\nRequirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (5.2.0)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (3.10.18)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.20->langchain_chroma) (4.24.0)\nRequirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.76->langchain_chroma) (0.4.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.76->langchain_chroma) (1.33)\nRequirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.76->langchain_chroma) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->langchain_chroma) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->langchain_chroma) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->langchain_chroma) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->langchain_chroma) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->langchain_chroma) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->langchain_chroma) (2.4.1)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (1.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.76->langchain_chroma) (3.0.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.25.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.40.3)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.8.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.32.5)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.3.1)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.5.0)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.10)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (0.23.0)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (6.32.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.13.1)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain_chroma) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.70.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.37.0)\nRequirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.37.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.20->langchain_chroma) (0.58b0)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (0.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (2.19.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (0.33.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (1.5.4)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.1)\nRequirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.0)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (15.0.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->langchain_chroma) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->langchain_chroma) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.0->langchain_chroma) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.0->langchain_chroma) (2024.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (4.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (2025.5.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (1.1.5)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain_chroma) (3.23.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.0->langchain_chroma) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (0.1.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.4.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.3.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (10.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.3.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.6.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.33)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.76)\nRequirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.107.2)\nRequirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\nRequirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (8.5.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (4.14.0)\nRequirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.11.7)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.5)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Groq API key loaded ✅\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from __future__ import annotations\nimport os, re, json, ast\nfrom typing import TypedDict, List, Dict, Any, Optional","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:32.702154Z","iopub.execute_input":"2025-09-14T06:03:32.702427Z","iopub.status.idle":"2025-09-14T06:03:32.706256Z","shell.execute_reply.started":"2025-09-14T06:03:32.702404Z","shell.execute_reply":"2025-09-14T06:03:32.705707Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# === Kaggle Secrets ===\ntry:\n    from kaggle_secrets import UserSecretsClient\n    secrets = UserSecretsClient()\n    os.environ[\"GROQ_API_KEY\"] = secrets.get_secret(\"GROQ_API_KEY\")\n    print(\"Groq API key loaded ✅\")\nexcept Exception as e:\n    print(\"[WARN] Could not load GROQ_API_KEY from Kaggle Secrets:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:32.706907Z","iopub.execute_input":"2025-09-14T06:03:32.707115Z","iopub.status.idle":"2025-09-14T06:03:32.786509Z","shell.execute_reply.started":"2025-09-14T06:03:32.707100Z","shell.execute_reply":"2025-09-14T06:03:32.785717Z"}},"outputs":[{"name":"stdout","text":"Groq API key loaded ✅\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# === LangChain / LLM ===\nfrom langchain_groq import ChatGroq\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\n\n# === Vector Memory ===\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\n\n\n# === LangGraph ===\nfrom langgraph.graph import StateGraph, END\n\n\n# === Search ===\nfrom ddgs import DDGS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:32.787442Z","iopub.execute_input":"2025-09-14T06:03:32.788152Z","iopub.status.idle":"2025-09-14T06:03:32.792549Z","shell.execute_reply.started":"2025-09-14T06:03:32.788129Z","shell.execute_reply":"2025-09-14T06:03:32.791767Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# ------------------------\n# GLOBALS / CONFIG\n# ------------------------\nLLM_MODEL = os.environ.get(\"GROQ_MODEL\", \"qwen/qwen3-32b\")\nTEMPERATURE = float(os.environ.get(\"LLM_TEMPERATURE\", \"0\"))\nMAX_REFLECTIONS = int(os.environ.get(\"MAX_REFLECTIONS\", \"2\"))\nMEM_COLLECTION = os.environ.get(\"MEM_COLLECTION\", \"mini_manus_memory\")\nEMBED_MODEL = os.environ.get(\"EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n\n\n# Persistent directory for Kaggle (working dir is saved in session)\nPERSIST_DIR = \"/kaggle/working/agent_memory\"\n\n\n# Initialize LLM (requires GROQ_API_KEY)\n_llm = ChatGroq(model=LLM_MODEL, temperature=TEMPERATURE)\n\n\n# Initialize vector memory\n_embedding_fn = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n_vectorstore = Chroma(\ncollection_name=MEM_COLLECTION,\nembedding_function=_embedding_fn,\npersist_directory=PERSIST_DIR\n)\n\n# ✅ DEFINE the variable, but leave it empty. We will create it in the loop.\n_vectorstore: Optional[Chroma] = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:32.794489Z","iopub.execute_input":"2025-09-14T06:03:32.794694Z","iopub.status.idle":"2025-09-14T06:03:33.847319Z","shell.execute_reply.started":"2025-09-14T06:03:32.794680Z","shell.execute_reply":"2025-09-14T06:03:33.846547Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nRAG_PERSIST_DIR = \"/kaggle/working/\"\nRAG_COLLECTION = \"rag_docs\"\n\ndef ingest_knowledge_base(file_path: str):\n    \"\"\"\n    Ingests a user-uploaded txt file into the knowledge base.\n    file_path: path to the uploaded text file\n    \"\"\"\n    print(f\"📂 Ingesting file: {file_path}\")\n\n    # Load the document\n    loader = TextLoader(file_path, encoding=\"utf-8\")\n    documents = loader.load()\n    \n    # Split into chunks\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n    chunks = text_splitter.split_documents(documents)\n    \n    print(f\"✅ File split into {len(chunks)} chunks.\")\n\n    # Create or update Chroma vector store\n    rag_db = Chroma.from_documents(\n        documents=chunks,\n        embedding=_embedding_fn,\n        persist_directory=RAG_PERSIST_DIR,\n        collection_name=RAG_COLLECTION\n    )\n\n    print(\"✅ Knowledge base updated and saved.\")\n    return f\"File '{file_path}' ingested successfully!\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.848102Z","iopub.execute_input":"2025-09-14T06:03:33.848363Z","iopub.status.idle":"2025-09-14T06:03:33.853892Z","shell.execute_reply.started":"2025-09-14T06:03:33.848326Z","shell.execute_reply":"2025-09-14T06:03:33.853141Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ------------------------\n# Memory helpers\n# ------------------------\ndef mem_add(text: str, kind: str = \"note\"):\n    \"\"\"Adds text to the vector store. Persistence is handled automatically.\"\"\"\n    _vectorstore.add_texts([f\"{kind}: {text}\"])\n    \n    # The erroneous .persist() line has been removed.\n    \n    # We can keep the debug print statement to see it working.\n    print(f\"🧠 Memory Add Request Sent: '{kind}: {text[:60]}...'\")\n\ndef mem_recall(query: str, k: int = 3):\n    \"\"\"Recalls k most similar documents from the vector store.\"\"\"\n    docs = _vectorstore.similarity_search(query, k=k)\n    return [d.page_content for d in docs]\n\n# Place this with your other node functions\n\n# Replace your global DIRECT_ANSWER_SYS variable with this:\n\nDIRECT_ANSWER_SYS = \"\"\"\nYou are a helpful assistant that answers questions ONLY from the provided memory context.\nThe memory may contain structured facts in JSON format, like {\"entity\": \"user\", \"attribute\": \"name\", \"value\": \"haad\"}.\n\n- To answer the user's question, you MUST parse these JSON facts.\n- When asked 'what is my name' or about the 'user', look for facts where 'entity' is 'user'.\n- When asked 'what is your name' or about the 'agent', look for facts where 'entity' is 'agent'.\n- Use this information to answer precisely. Do not mention the JSON structure in your answer.\n\n- If the memory does NOT contain enough information to answer, you MUST respond with the exact phrase: 'NO_DIRECT_ANSWER' and nothing else.\n\"\"\"\n\ndef node_direct_answer(state: GraphState) -> GraphState:\n    \"\"\"Checks for a direct answer in memory before planning.\"\"\"\n    # ✅ CHANGE: Get the log and append actions\n    log = state.get(\"log\", [])\n    \n    mem_list = mem_recall(state.get(\"user_input\", \"\"), k=5)\n    facts = mem_recall(\"fact:\", k=5)\n    mem_list.extend(facts)\n    mem_list = list(set(mem_list))\n    mem_text = \"\\n\".join(mem_list) if mem_list else \"<none>\"\n\n    prompt = (DIRECT_ANSWER_SYS + \"\\nRelevant memory:\\n\" + mem_text + \"\\nUser input:\\n\" + state.get(\"user_input\", \"\"))\n    response = _llm.invoke(prompt).content.strip()\n\n    if \"NO_DIRECT_ANSWER\" in response:\n        log.append(\"🤔 Direct Answer: No direct answer found in memory. Proceeding to planner.\")\n        return {\"log\": log}\n    else:\n        log.append(\"✅ Direct Answer: Found a direct answer in memory. Finalizing.\")\n        return {\"final\": response, \"log\": log}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.854599Z","iopub.execute_input":"2025-09-14T06:03:33.854823Z","iopub.status.idle":"2025-09-14T06:03:33.873674Z","shell.execute_reply.started":"2025-09-14T06:03:33.854802Z","shell.execute_reply":"2025-09-14T06:03:33.872958Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ------------------------\n# Safe JSON utilities\n# ------------------------\ndef extract_json_block(text: str) -> Optional[str]:\n    import re\n    match = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, flags=re.S)\n    return match.group(1) if match else None\n\n\ndef safe_json_loads(text: str) -> Any:\n    try:\n        return json.loads(text)\n    except Exception:\n        block = extract_json_block(text)\n        if block:\n            try:\n                cleaned = block.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n                return json.loads(cleaned)\n            except Exception as e:\n                return {\"error\": f\"json parse fail: {e}\", \"raw\": block}\n        return {\"error\": \"no json found\", \"raw\": text}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.874414Z","iopub.execute_input":"2025-09-14T06:03:33.874632Z","iopub.status.idle":"2025-09-14T06:03:33.890112Z","shell.execute_reply.started":"2025-09-14T06:03:33.874610Z","shell.execute_reply":"2025-09-14T06:03:33.889455Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# ------------------------\n# Tools (web_search, calculator)\n# ------------------------\n\nfrom ddgs import DDGS\n\ndef tool_web_search(query: str, max_results: int = 5) -> Dict[str, Any]:\n    results: List[Dict[str, Any]] = []\n    try:\n        with DDGS() as ddgs:\n            for r in ddgs.text(query, max_results=max_results):\n                results.append({\n                    \"title\": r.get(\"title\"),\n                    \"href\": r.get(\"href\"),\n                    \"body\": r.get(\"body\"),\n                })\n    except Exception as e:\n        return {\"error\": f\"search_failed: {e}\"}\n    return {\"results\": results}\n\n\nclass _MathVisitor(ast.NodeVisitor):\n    allowed_nodes = (\n        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Mod, ast.Pow,\n        ast.USub, ast.UAdd, ast.FloorDiv\n    )\n    def visit(self, node):\n        if not isinstance(node, self.allowed_nodes):\n            raise ValueError(f\"disallowed expression: {type(node).__name__}\")\n        return super().visit(node)\n\n\ndef tool_calculator(expression: str) -> Dict[str, Any]:\n    try:\n        node = ast.parse(expression, mode=\"eval\")\n        _MathVisitor().visit(node)\n        value = eval(compile(node, \"<calc>\", \"eval\"), {\"__builtins__\": {}}, {})\n        return {\"ok\": True, \"result\": value}\n    except Exception as e:\n        return {\"ok\": False, \"error\": str(e)}\n\n\n# ===============================================================\n# ✅ Update tool_rag_search to always use the latest persisted DB\n# ===============================================================\ndef tool_rag_search(query: str) -> Dict[str, Any]:\n    try:\n        rag_db = Chroma(\n            persist_directory=RAG_PERSIST_DIR,\n            embedding_function=_embedding_fn,\n            collection_name=RAG_COLLECTION\n        )\n        docs = rag_db.similarity_search(query, k=3)\n        results_text = \"\\n---\\n\".join([d.page_content for d in docs])\n        return {\"results\": results_text or \"No relevant info found in the uploaded file.\"}\n    except Exception as e:\n        return {\"error\": f\"rag_search_failed: {e}\"}\n\n        \n\nTOOLS = {\n    \"web_search\": {\n        \"desc\": \"Search the web for general information, current events, or real-world people and places.\",\n        \"func\": lambda args: tool_web_search(args.get(\"query\", \"\"), int(args.get(\"max_results\", 3)))\n    },\n    \"calculator\": {\n        \"desc\": \"Evaluate arithmetic expressions.\",\n        \"func\": lambda args: tool_calculator(args.get(\"expression\", \"\"))\n    },\n    # ✅ UPDATE THIS DESCRIPTION\n    \"rag_search\": {\n        \"desc\": \"Use this tool to answer questions about the fantasy story 'The Lantern of Aetheria'. It contains specific knowledge about characters like Kael and Elara, and places like the city of Aetheria.\",\n        \"func\": lambda args: tool_rag_search(args.get(\"query\", \"\"))\n    }\n}\n\n# # This will automatically update the string passed to the Planner\n# TOOL_LIST_STR = \"\\n\".join([f\"- {name}: {meta['desc']}\" for name, meta in TOOLS.items()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.890941Z","iopub.execute_input":"2025-09-14T06:03:33.891161Z","iopub.status.idle":"2025-09-14T06:03:33.907666Z","shell.execute_reply.started":"2025-09-14T06:03:33.891146Z","shell.execute_reply":"2025-09-14T06:03:33.907137Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# ------------------------\n# Graph State\n# ------------------------\n# In your GraphState TypedDict definition\n\nclass GraphState(TypedDict, total=False):\n    user_input: str\n    memory_context: str\n    plan: List[Dict[str, Any]]\n    observations: List[Dict[str, Any]]\n    draft: str\n    feedback: str\n    reflections: int\n    final: str\n    # ✅ ADD THIS LINE: This will be our flight recorder.\n    log: List[str]\n\n\n# ------------------------\n# Node: Planner\n# ------------------------\n\n# ✅ NEW: A prompt specifically for understanding the user's true intent.\n# Replace your old INTENT_DISTILLER_PROMPT with this one\nINTENT_DISTILLER_PROMPT = \"\"\"\nYou are an intent distiller. Your job is to analyze the conversation and determine the user's real, actionable task.\n- If the user has provided a file, the primary task is to process that file using the instructions in the user's text.\n- If the user refers to a specific file (e.g., \"in summary.txt\"), separate the core question from the file reference.\n\nYour output should be a clear, actionable instruction for the Planner AI.\n\nExample 1:\nUser input: \"summarize the main points of the attached file\"\nFile Path: \"/path/to/doc.txt\"\nResult: \"The user has uploaded a file at /path/to/doc.txt and wants a summary. The first step is to add the file to the knowledge base, then search it for the main points.\"\n\nExample 2:\nUser input: \"whats the poem name in summary.txt\"\nFile Path: null\nResult: \"The user wants to know the name of the poem inside the file 'summary.txt'. The task is to search within that specific file for the poem's title.\"\n\nNow, distill the intent from the following:\n\"\"\"\n\n# Replace your old PLANNER_SYS prompt with this corrected version\n\n# ✅ THIS IS THE CORRECTED PROMPT. PLEASE REPLACE THE OLD ONE WITH THIS.\n\n# ✅ STEP 1: Replace your old PLANNER_SYS variable with this one.\n\nPLANNER_SYS = \"\"\"\nYou are the Planner. Your goal is to create a step-by-step plan to answer the user's task.\n**VERY IMPORTANT: Before creating a new plan, check 'Relevant memory'. If the answer is already there, your plan should be a single step to state the answer directly without using tools.**\n\nAvailable tools:\n{tool_list}\n\n**CRITICAL RULES for rag_search:**\n1.  When the user asks about a specific file (e.g., \"in summary.txt\"), you MUST use the `rag_search` tool with the `source_file` argument set to the filename (e.g., \"summary.txt\").\n2.  When using `source_file`, the `query` argument MUST be a question that represents the user's core goal. **DO NOT leave the query empty.** Reformulate the user's request into a proper question for the search.\n\n**Example of a good plan:**\nUser Task: \"The user wants to know the name of the poem inside the file 'summary.txt'.\"\nCorrect Plan:\n{\n  \"steps\": [\n    {\n      \"id\": 1,\n      \"thought\": \"I need to find the name of the poem inside 'summary.txt'. I will use rag_search and filter by the source file. I will also formulate a query to find the poem's name.\",\n      \"tool\": \"rag_search\",\n      \"args\": {\n        \"query\": \"What is the name of the poem?\",\n        \"source_file\": \"summary.txt\"\n      },\n      \"output_key\": \"poem_content\"\n    }\n  ]\n}\n\nReturn a STRICT JSON array named 'steps'.\n\"\"\"\n\n# ✅ STEP 2: Replace your entire old node_planner function with this one.\n\ndef node_planner(state: GraphState) -> GraphState:\n    log = state.get(\"log\", [])\n    user_input = state.get(\"user_input\", \"\")\n    file_path = state.get(\"file_path\")\n\n    # --- Intent Distiller Step ---\n    mem_list = mem_recall(user_input, k=4)\n    mem_text = \"\\n\".join(mem_list) if mem_list else \"<none>\"\n    file_info = f\"\\nFile Path: \\\"{file_path}\\\"\" if file_path else \"\\nFile Path: null\"\n\n    distiller_prompt = (\n        INTENT_DISTILLER_PROMPT\n        + \"\\nRelevant memory:\\n\" + mem_text\n        + \"\\nUser input:\\n\" + user_input\n        + file_info\n    )\n\n    distilled_task = _llm.invoke(distiller_prompt).content.strip()\n    log.append(f\"🎯 Planner: Distilled user intent to: '{distilled_task}'\")\n\n    # --- Planning Step ---\n    tool_list_str = \"\\n\".join([f\"- {name}: {meta['desc']}\" for name, meta in TOOLS.items()])\n    \n    # ✅ CHANGED: We now use the safe .replace() method instead of .format()\n    planner_prompt_template = PLANNER_SYS.replace(\"{tool_list}\", tool_list_str)\n\n    prompt = (\n        planner_prompt_template\n        + \"\\nRelevant memory (may be empty):\\n\" + mem_text\n        + \"\\nUser task:\\n\" + distilled_task\n        + \"\\nRespond with ONLY JSON in the format: {\\\"steps\\\":[...] }\\n\"\n    )\n    raw = _llm.invoke(prompt).content.strip()\n    parsed = safe_json_loads(raw)\n    steps = parsed.get(\"steps\") if isinstance(parsed, dict) and isinstance(parsed.get(\"steps\"), list) else []\n\n    log.append(f\"📝 Planner: Generated a plan with {len(steps)} step(s).\")\n    return {\"plan\": steps, \"log\": log}\n\n\n\n# ------------------------\n# Node: Executors\n# ------------------------\nEXECUTOR_SYS = \"\"\"\nYou are the Executor. Given the user's request, the plan, and tool observations,\nwrite a clear, helpful draft answer. If observations include search results, cite them inline textually (titles/domains), but do not fabricate links.\nIf no tools were used, answer from general knowledge + memory context. Keep it concise unless the user asked for depth.\n\"\"\"\n\n\ndef _run_tools(steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    observations: List[Dict[str, Any]] = []\n    for step in steps:\n        tool_name = step.get(\"tool\")\n        args = step.get(\"args\", {}) or {}\n        if tool_name and tool_name in TOOLS:\n            try:\n                obs = TOOLS[tool_name][\"func\"](args)\n            except Exception as e:\n                obs = {\"error\": f\"tool_error: {e}\"}\n        else:\n            obs = {\"note\": \"no_tool\"}\n        observations.append({\n            \"id\": step.get(\"id\"),\n            \"tool\": tool_name,\n            \"args\": args,\n            \"observation\": obs,\n        })\n    return observations\n\n\ndef node_executor(state: GraphState) -> GraphState:\n    \"\"\"Runs tools and generates a draft answer.\"\"\"\n    # ✅ CHANGE: Get the log and append actions\n    log = state.get(\"log\", [])\n    plan = state.get(\"plan\", [])\n    \n    tool_steps = [step for step in plan if step.get(\"tool\")]\n    if tool_steps:\n        tools_str = \", \".join([step['tool'] for step in tool_steps])\n        log.append(f\"🛠️ Executor: Running tool(s): {tools_str}\")\n    else:\n        log.append(\"🧠 Executor: No tools to run. Answering from general knowledge.\")\n\n    observations = []\n    for step in plan:\n        tool_name = step.get(\"tool\")\n        args = step.get(\"args\", {}) or {}\n        if tool_name and tool_name in TOOLS:\n            obs = TOOLS[tool_name][\"func\"](args)\n        else:\n            obs = {\"note\": \"no_tool\"}\n        observations.append({\"id\": step.get(\"id\"), \"tool\": tool_name, \"args\": args, \"observation\": obs})\n\n    context_blob = json.dumps({\"plan\": plan, \"observations\": observations, \"user_input\": state.get(\"user_input\", \"\")}, ensure_ascii=False)\n    draft_prompt = \"You are the Executor... \\nContext JSON:\\n\" + context_blob + \"\\nDraft the answer now.\"\n    draft = _llm.invoke(draft_prompt).content.strip()\n    \n    return {\"observations\": observations, \"draft\": draft, \"log\": log}\n\n\n# ------------------------\n# Node: Verifier (reflection loop)\n# ------------------------\nVERIFIER_SYS = \"\"\"\nYou are the Verifier. Check the draft for factuality, clarity, safety, and task completion.\nReturn ONLY JSON: {\"approved\": bool, \"feedback\": \"...\", \"final\": \"...\"}\n- If approved: polish the draft lightly and place in 'final'.\n- If NOT approved: explain issues in 'feedback' and leave 'final' empty.\nBe conservative; prefer one more revision if unsure.\n\"\"\"\n\n\ndef node_verifier(state: GraphState) -> GraphState:\n    \"\"\"Verifies the draft answer.\"\"\"\n    # ✅ CHANGE: Get the log and append actions\n    log = state.get(\"log\", [])\n    \n    prompt = (VERIFIER_SYS + \"\\nUser input:\\n\" + state.get(\"user_input\", \"\") + \"\\nDraft:\\n\" + (state.get(\"draft\", \"\") or \"\") + \"\\nObservations (for verification):\\n\" + json.dumps(state.get(\"observations\", []), ensure_ascii=False))\n    raw = _llm.invoke(prompt).content.strip()\n    parsed = safe_json_loads(raw)\n    \n    if isinstance(parsed, dict) and parsed.get(\"approved\") and parsed.get(\"final\"):\n        log.append(\"✅ Verifier: Draft approved.\")\n        return {\"final\": parsed.get(\"final\"), \"log\": log}\n\n    fb = parsed.get(\"feedback\", \"\") if isinstance(parsed, dict) else \"needs another pass\"\n    log.append(f\"❌ Verifier: Draft not approved. Feedback: '{fb[:50]}...'. Replanning.\")\n    mem_add(f\"verifier_feedback: {fb}\", kind=\"feedback\")\n    return {\"feedback\": fb, \"reflections\": (state.get(\"reflections\", 0) + 1), \"log\": log}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.908384Z","iopub.execute_input":"2025-09-14T06:03:33.908622Z","iopub.status.idle":"2025-09-14T06:03:33.926774Z","shell.execute_reply.started":"2025-09-14T06:03:33.908599Z","shell.execute_reply":"2025-09-14T06:03:33.926251Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ------------------------\n# Node: Memory Updater (end)\n# ------------------------\n\ndef node_memory_update(state: GraphState) -> GraphState:\n    \"\"\"Saves the conversation and extracts structured facts to memory.\"\"\"\n    log = state.get(\"log\", [])\n    log.append(\"💾 Memory: Saving final answer and user query to long-term memory.\")\n    \n    ui = state.get(\"user_input\", \"\")\n    final = state.get(\"final\", \"\")\n    if ui: mem_add(ui, kind=\"user\")\n    if final: mem_add(final, kind=\"agent\")\n\n    # ✅ CHANGE: Upgraded to structured JSON fact extraction\n    # This block replaces the old, simple fact extraction.\n    low = ui.lower()\n    \n    # For the user's name\n    if \"my name is\" in low:\n        name = low.split(\"my name is\", 1)[-1].strip().strip(\".?! \")\n        if name:\n            fact_json = json.dumps({\"entity\": \"user\", \"attribute\": \"name\", \"value\": name})\n            mem_add(fact_json, kind=\"fact\")\n            \n    # For the agent's name\n    if \"your name is\" in low or \"ur name is\" in low:\n        name = low.split(\" is \", 1)[-1].strip().strip(\".?! \")\n        if name:\n            fact_json = json.dumps({\"entity\": \"agent\", \"attribute\": \"name\", \"value\": name})\n            mem_add(fact_json, kind=\"fact\")\n\n    # For other facts, like a brother's name\n    if \"my brother name is\" in low:\n        name = low.split(\"my brother name is\", 1)[-1].strip().strip(\".?! \")\n        if name:\n            fact_json = json.dumps({\"entity\": \"user's brother\", \"attribute\": \"name\", \"value\": name})\n            mem_add(fact_json, kind=\"fact\")\n\n    return {\"log\": log}\n\ndef finalizer(state: GraphState) -> GraphState:\n    final_answer = state.get(\"draft\") or \"[Agent had no result]\"\n    return {**state, \"final\": final_answer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.927482Z","iopub.execute_input":"2025-09-14T06:03:33.927679Z","iopub.status.idle":"2025-09-14T06:03:33.944928Z","shell.execute_reply.started":"2025-09-14T06:03:33.927664Z","shell.execute_reply":"2025-09-14T06:03:33.944397Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# ------------------------\n# Graph assembly\n# ------------------------\n\ndef run_agent_once(user_input: str) -> Dict[str, Any]:\n    global _vectorstore\n    _vectorstore = Chroma(\n        collection_name=MEM_COLLECTION,\n        embedding_function=_embedding_fn,\n        persist_directory=PERSIST_DIR\n    )\n    graph = build_graph()\n    # ✅ CHANGE: Initialize the log in the state\n    state: GraphState = {\"user_input\": user_input, \"reflections\": 0, \"log\": []}\n    result = graph.invoke(state)\n    return result\n\n    \n# Place this with your _should_reflect function\n# ✅ THIS IS THE MISSING FUNCTION\ndef _should_reflect(state: GraphState) -> str:\n    if state.get(\"final\"):\n        return \"approved\"\n    if int(state.get(\"reflections\", 0)) >= MAX_REFLECTIONS:\n        return \"give_up\"\n    return \"replan\"\n\ndef should_plan_or_finish(state: GraphState) -> str:\n    \"\"\"Decides whether to plan or end the process.\"\"\"\n    if state.get(\"final\"):\n        # A direct answer was found by the previous node.\n        return \"finish\"\n    else:\n        # No direct answer, so we must proceed to planning.\n        return \"plan\"\n        \n# Replace your build_graph function with this final version\n\ndef build_graph():\n    workflow = StateGraph(GraphState)\n\n    # Add the new node\n    workflow.add_node(\"direct_answer\", node_direct_answer)\n    \n    workflow.add_node(\"planner\", node_planner)\n    workflow.add_node(\"executor\", node_executor)\n    workflow.add_node(\"verifier\", node_verifier)\n    workflow.add_node(\"finalizer\", finalizer)\n    workflow.add_node(\"memory\", node_memory_update)\n\n    # The entry point is now our direct_answer node\n    workflow.set_entry_point(\"direct_answer\")\n\n    # Add the new conditional edge\n    workflow.add_conditional_edges(\n        \"direct_answer\",\n        should_plan_or_finish,\n        {\n            \"finish\": \"memory\", # If we have an answer, just save the convo and end.\n            \"plan\": \"planner\"   # If no answer, start the main planning loop.\n        }\n    )\n    \n    # This is the original agent loop\n    workflow.add_edge(\"planner\", \"executor\")\n    workflow.add_edge(\"executor\", \"verifier\")\n    workflow.add_conditional_edges(\n        \"verifier\",\n        _should_reflect,\n        {\n            \"replan\": \"planner\",\n            \"approved\": \"memory\",\n            \"give_up\": \"finalizer\"\n        }\n    )\n    workflow.add_edge(\"finalizer\", \"memory\")\n    workflow.add_edge(\"memory\", END)\n\n    return workflow.compile()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.945658Z","iopub.execute_input":"2025-09-14T06:03:33.945884Z","iopub.status.idle":"2025-09-14T06:03:33.964454Z","shell.execute_reply.started":"2025-09-14T06:03:33.945864Z","shell.execute_reply":"2025-09-14T06:03:33.963912Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ------------------------\n# Public API\n# ------------------------\ndef run_agent_once(user_input: str) -> Dict[str, Any]:\n    global _vectorstore\n    \n    # ✅ THE FIX: Create a new Chroma instance, loading the latest data from disk.\n    # Assign it to the global variable so all your node functions can use it.\n    _vectorstore = Chroma(\n        collection_name=MEM_COLLECTION,\n        embedding_function=_embedding_fn,\n        persist_directory=PERSIST_DIR\n    )\n    \n    graph = build_graph()\n    state: GraphState = {\"user_input\": user_input, \"reflections\": 0}\n    result = graph.invoke(state)\n    return result\n\n\nif __name__ == \"__main__\":\n    if not os.environ.get(\"GROQ_API_KEY\"):\n        print(\"[WARN] GROQ_API_KEY is not set. Set it before running for LLM calls.\")\n    demo_q = \"Give me 3 bullet points on why AI agents are useful for students.\"\n    out = run_agent_once(demo_q)\n    print(\"\\n=== FINAL ANSWER ===\\n\", out.get(\"final\"))\n    print(\"\\n--- Debug state keys ---\\n\", list(out.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:33.965192Z","iopub.execute_input":"2025-09-14T06:03:33.965463Z","iopub.status.idle":"2025-09-14T06:03:34.820088Z","shell.execute_reply.started":"2025-09-14T06:03:33.965441Z","shell.execute_reply":"2025-09-14T06:03:34.819512Z"}},"outputs":[{"name":"stdout","text":"🧠 Memory Add Request Sent: 'user: Give me 3 bullet points on why AI agents are useful for stud...'\n🧠 Memory Add Request Sent: 'agent: <think>\nOkay, the user is asking for three bullet points on ...'\n\n=== FINAL ANSWER ===\n <think>\nOkay, the user is asking for three bullet points on why AI agents are useful for students. Let me start by recalling the common benefits of AI in education. Personalized learning is a big one because AI can adapt to each student's needs. Then, instant feedback is important for quick corrections. Also, 24/7 availability helps students get help anytime. I need to make sure these points are clear and concise. Let me check if there's any specific information in the memory context, but the user mentioned the memory is empty. So I can proceed with these general points. Alright, time to structure them into bullet points.\n</think>\n\n- **Personalized Learning:** AI agents adapt to individual learning styles and paces, offering tailored resources and explanations to enhance understanding.  \n- **Instant Feedback:** They provide real-time corrections and suggestions, helping students identify and address mistakes quickly.  \n- **24/7 Accessibility:** AI agents are available anytime, offering support for homework, clarifying doubts, or practicing skills outside traditional classroom hours.\n\n--- Debug state keys ---\n ['user_input', 'reflections', 'final', 'log']\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# =======================\n# Chat Loop for Agent\n# =======================\ndef chat_loop():\n    print(\"🤖 Agent ready! Type 'exit' to quit.\\n\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.strip().lower() in [\"exit\", \"quit\", \"q\"]:\n            print(\"👋 Goodbye!\")\n            break\n\n        result = run_agent_once(user_input)\n        \n        # ✅ CHANGE: Added the block to print the log\n        print(\"\\n--- 🕵️ Agent's Thought Process ---\")\n        log = result.get(\"log\", [])\n        for step in log:\n            print(f\"➡️ {step}\")\n        print(\"---------------------------------\")\n        \n        print(\"\\nAgent:\\n\", result.get(\"final\", \"[No final output]\"))\n\n        recalls = mem_recall(user_input, k=2)\n        if recalls:\n            print(\"\\n💾 Memory recall:\\n\", \"\\n\".join(recalls))\n\n# === Run chat ===\nif __name__ == \"__main__\":\n    chat_loop()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:34.822080Z","iopub.execute_input":"2025-09-14T06:03:34.822320Z","iopub.status.idle":"2025-09-14T06:06:22.248229Z","shell.execute_reply.started":"2025-09-14T06:03:34.822303Z","shell.execute_reply":"2025-09-14T06:06:22.247600Z"}},"outputs":[{"name":"stdout","text":"🤖 Agent ready! Type 'exit' to quit.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Hello ! Whats the weather today\n"},{"name":"stdout","text":"🧠 Memory Add Request Sent: 'user: Hello ! Whats the weather today...'\n🧠 Memory Add Request Sent: 'agent: Here's the current weather information:\n\n**Des Moines, IA To...'\n\n--- 🕵️ Agent's Thought Process ---\n➡️ 🤔 Direct Answer: No direct answer found in memory. Proceeding to planner.\n➡️ 🎯 Planner: Distilled user intent to: '<think>\nOkay, let's see. The user said, \"Hello! What's the weather today?\" and there's no file path provided. My job is to figure out the real, actionable task here.\n\nFirst, the user is asking about the weather. That's straightforward. They want to know the current weather conditions. Since there's no file mentioned, the task isn't about processing a file. The greeting \"Hello!\" is just a polite opener, so the main request is the weather information.\n\nI need to make sure there's no hidden task. The user didn't mention any files or specific data to process. So the primary action is to provide the current weather details. The Planner AI should be instructed to fetch and present the weather data for today. No file handling is needed here. Just a simple query about the weather. Alright, that's the distilled intent.\n</think>\n\nThe user is asking for the current weather information. The task is to provide a concise summary of today's weather conditions.'\n➡️ 📝 Planner: Generated a plan with 1 step(s).\n➡️ 🛠️ Executor: Running tool(s): web_search\n➡️ ✅ Verifier: Draft approved.\n➡️ 💾 Memory: Saving final answer and user query to long-term memory.\n---------------------------------\n\nAgent:\n Here's the current weather information:\n\n**Des Moines, IA Today:**\n- **Conditions:** Partly Cloudy\n- **Temperature:** 78°F\n- **Humidity:** 65%\n- **Wind:** SW 4 mph\n- **Feels Like:** 78°F\n\n**Regional Notes:**\n- Heat records may be challenged with 90-100°F in the Mississippi Valley.\n- Potential tropical development is being monitored in the Gulf.\n\nWould you like weather details for a different location?\n\n💾 Memory recall:\n user: Hello ! Whats the weather today\nagent: Here's the current weather information:\n\n**Des Moines, IA Today:**\n- **Conditions:** Partly Cloudy\n- **Temperature:** 78°F\n- **Humidity:** 65%\n- **Wind:** SW 4 mph\n- **Feels Like:** 78°F\n\n**Regional Notes:**\n- Heat records may be challenged with 90-100°F in the Mississippi Valley.\n- Potential tropical development is being monitored in the Gulf.\n\nWould you like weather details for a different location?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  quit\n"},{"name":"stdout","text":"👋 Goodbye!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# The text of your story\nknowledge_base_text = \"\"\"\n\"Moonlit Night\"\n\nThe stars shine bright in the midnight sky\nA gentle breeze whispers by\nThe world is hushed, in quiet sleep\nAs the moon's soft light begins to creep\n\nThe shadows dance upon the wall\nA midnight serenade, for one and all\nThe moon's sweet beams, illuminate the night\nA peaceful scene, a wondrous sight.\n\"\"\"\n\n# The Fix: Use a relative path to save the file in /kaggle/working/\nfile_path = \"Moonlit.txt\"\nwith open(file_path, \"w\") as f:\n    f.write(knowledge_base_text)\n\nprint(f\"✅ '{file_path}' created successfully in /kaggle/working/.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:06:22.248988Z","iopub.execute_input":"2025-09-14T06:06:22.249263Z","iopub.status.idle":"2025-09-14T06:06:22.254361Z","shell.execute_reply.started":"2025-09-14T06:06:22.249240Z","shell.execute_reply":"2025-09-14T06:06:22.253724Z"}},"outputs":[{"name":"stdout","text":"✅ 'Moonlit.txt' created successfully in /kaggle/working/.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import gradio as gr\nimport markdown\n\n# Function to handle file ingestion\ndef upload_file(file):\n    if file is None:\n        return \"⚠️ Please upload a .txt file.\"\n    return ingest_knowledge_base(file.name)\n\n# Function for chat\ndef run_agent_for_ui(user_input, history):\n    print(f\"Received input: {user_input}\")\n    result = run_agent_once(user_input)\n\n    # Format output\n    log_steps = result.get(\"log\", [])\n    final_answer = result.get(\"final\", \"I'm sorry, I encountered an error.\")\n    \n    log_html = \"<details><summary>🕵️ Click to see Agent's Thought Process</summary><ul>\"\n    for step in log_steps:\n        safe_step = markdown.markdown(step.replace(\"\\n\", \"<br>\"))\n        log_html += f\"<li>{safe_step}</li>\"\n    log_html += \"</ul></details>\"\n\n    return f\"{final_answer}\\n\\n{log_html}\"\n\nprint(\"Launching Gradio UI...\")\n\n# Build UI with file upload + chat\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## 📘 General AI Agent with Custom Knowledge Base\")\n    \n    with gr.Tab(\"📂 Upload File\"):\n        file_input = gr.File(label=\"Upload a TXT file\", file_types=[\".txt\"])\n        upload_output = gr.Textbox(label=\"Ingestion Status\")\n        file_input.change(upload_file, inputs=file_input, outputs=upload_output)\n    \n    with gr.Tab(\"💬 Chat with Agent\"):\n        gr.ChatInterface(\n            fn=run_agent_for_ui,\n            title=\"Chat with AI Agent\",\n            description=\"You can now query on your uploaded file.\"\n        )\n\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:06:22.255014Z","iopub.execute_input":"2025-09-14T06:06:22.255282Z"}},"outputs":[{"name":"stdout","text":"Launching Gradio UI...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  self.chatbot = Chatbot(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://a54a319b1fb9ef6b0b.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://a54a319b1fb9ef6b0b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Received input: Hello \n🧠 Memory Add Request Sent: 'user: Hello ...'\n🧠 Memory Add Request Sent: 'agent: <think>\nOkay, the user said \"Hello !\" and then asked about t...'\nReceived input: what do you have today for me \n🧠 Memory Add Request Sent: 'user: what do you have today for me ...'\n🧠 Memory Add Request Sent: 'agent: Today, I can assist you with:\n\n1. **Weather updates** – Curr...'\nReceived input: General web searches\n🧠 Memory Add Request Sent: 'user: General web searches...'\n🧠 Memory Add Request Sent: 'agent: <think>\nOkay, the user is asking for three bullet points on ...'\nReceived input: Calculations\n🧠 Memory Add Request Sent: 'user: Calculations...'\n🧠 Memory Add Request Sent: 'agent: I'm ready to help with calculations! Please provide the math...'\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}