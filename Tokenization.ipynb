{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOOg1Co5s1JtSlfuVm9sc+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilmulla16/LLM-from-Scratch/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Creating Tokens**"
      ],
      "metadata": {
        "id": "kkoNKGgheSb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/J. K. Rowling - Harry Potter 1 - Sorcerer\\'s Stone.txt', 'r' , encoding=\"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "print(\"Total No of Characters\" , len(raw_text))\n",
        "print(raw_text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYtIoOtoedsB",
        "outputId": "b4d1e6f1-86f3-4e60-f07c-b3b14aa68e7c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total No of Characters 439742\n",
            "Harry Potter and the Sorcerer's Stone\n",
            "\n",
            "\n",
            "CHAPTER ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello. how are you doing! as of now?\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJtUT4X6gRZo",
        "outputId": "82984103-54c8-4d9d-8cfa-f5490e068661"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello.', ' ', 'how', ' ', 'are', ' ', 'you', ' ', 'doing!', ' ', 'as', ' ', 'of', ' ', 'now?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([.,!?] |\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Aeivirh3-k",
        "outputId": "71d89c07-fa53-44b1-d7da-1d5c1dfd737f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '. ', 'how', ' ', 'are', ' ', 'you', ' ', 'doing', '! ', 'as', ' ', 'of', ' ', 'now?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip() ]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjdexUBMjg4S",
        "outputId": "0c5963f6-e596-48ea-b97b-13f574f61fdc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '. ', 'how', 'are', 'you', 'doing', '! ', 'as', 'of', 'now?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, how' are you doing! as-- of? now?\"\n",
        "\n",
        "result = re.split(r'([,!:?_;()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR-7pU3DwTf5",
        "outputId": "1140f824-5c21-4a2b-8ab8-4dc655a32540"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'how', \"'\", 'are', 'you', 'doing', '!', 'as', '--', 'of', '?', 'now', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,!:?_;()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ieo8enznzTu9",
        "outputId": "76d271f4-4485-4523-b564-0b55ee0cbd11"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'\", 's', 'Stone', 'CHAPTER', 'ONE', 'THE', 'BOY', 'WHO', 'LIVED', 'Mr.', 'and', 'Mrs.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much.', 'They', 'were', 'the', 'last', 'people', 'you', \"'\", 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'didn', \"'\", 't', 'hold', 'with', 'such', 'nonsense.', 'Mr.', 'Dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'Grunnings', ',', 'which', 'made', 'drills.', 'He', 'was', 'a', 'big', ',', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', ',', 'although', 'he', 'did', 'have', 'a', 'very']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w_kOq8Oz0fP",
        "outputId": "5b1aa54e-94bc-490f-dd1a-1b4909d39365"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2 : Creating Tokens IDs**\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "1S6OnhpK0Yxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "print(len(all_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuqsHAES0l0Y",
        "outputId": "501690cc-ef1a-430f-871f-0058722ce59e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n"
      ],
      "metadata": {
        "id": "ApGhZ6q5_Fn7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i , item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRK8yULG_XKO",
        "outputId": "eb123d30-7115-48e6-ce6d-d5c4862ff517"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('\"-', 2)\n",
            "('\"...', 3)\n",
            "('\"...for', 4)\n",
            "('\"0', 5)\n",
            "('\"A', 6)\n",
            "('\"AAAAAAAAAARGH', 7)\n",
            "('\"AAAARGH', 8)\n",
            "('\"ALL', 9)\n",
            "('\"ARE', 10)\n",
            "('\"Aaah', 11)\n",
            "('\"Aargh', 12)\n",
            "('\"Abbott', 13)\n",
            "('\"Abou', 14)\n",
            "('\"About', 15)\n",
            "('\"Absolutely', 16)\n",
            "('\"After', 17)\n",
            "('\"Ah', 18)\n",
            "('\"Aha', 19)\n",
            "('\"Ahem', 20)\n",
            "('\"Ahern', 21)\n",
            "('\"Alas', 22)\n",
            "('\"All', 23)\n",
            "('\"Always', 24)\n",
            "('\"Am', 25)\n",
            "('\"An', 26)\n",
            "('\"And', 27)\n",
            "('\"Another', 28)\n",
            "('\"Anyone', 29)\n",
            "('\"Anything', 30)\n",
            "('\"Anything.', 31)\n",
            "('\"Anyway', 32)\n",
            "('\"Are', 33)\n",
            "('\"Aren', 34)\n",
            "('\"As', 35)\n",
            "('\"Asked', 36)\n",
            "('\"At', 37)\n",
            "('\"Ate', 38)\n",
            "('\"B-b-but', 39)\n",
            "('\"B-but', 40)\n",
            "('\"Back', 41)\n",
            "('\"Bad', 42)\n",
            "('\"Barking', 43)\n",
            "('\"Be', 44)\n",
            "('\"Because', 45)\n",
            "('\"Best', 46)\n",
            "('\"Bet', 47)\n",
            "('\"Better', 48)\n",
            "('\"Bin', 49)\n",
            "('\"Blasted', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "rheR3cjp_7p1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the tokenizer\n"
      ],
      "metadata": {
        "id": "al1_3_KFLs2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"THE BOY WHO LIVED\"\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDJeX6pfL0LW",
        "outputId": "a9380fc9-8445-4c32-9681-7ee858dd69b6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1775, 695, 1899, 1273]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "juYcC1U6PD_p",
        "outputId": "c42cc91f-5617-414e-a0ee-9f3a18377a97"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'THE BOY WHO LIVED'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello , do you like tea\"\n",
        "result = tokenizer.encode(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "wJI_z8C2QOPA",
        "outputId": "30a9f9e3-513e-4a58-a6ca-a973c7fe744d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3456208988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello , do you like tea\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1902381733.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Special Context Tokens\n",
        "\n",
        "> Reason because of the key error getting due to the new words.\n",
        "\n"
      ],
      "metadata": {
        "id": "e8JhZ7X0w11x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_token = sorted(list(set(preprocessed)))\n",
        "all_token.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_token)}"
      ],
      "metadata": {
        "id": "uTbSYuovRsm7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spfbWUP70AjD",
        "outputId": "2c3b7682-333c-4a46-f2a9-8ac9d001a99c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9058"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,item in  enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GHe-5_500GK",
        "outputId": "37261a14-6190-44c4-d00c-f5760b62b0b8"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('zoom', 9053)\n",
            "('zoomed', 9054)\n",
            "('zooming', 9055)\n",
            "('<|unk|>', 9056)\n",
            "('<|endoftext|>', 9057)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        # added this to replace the new text or unkown text with this <|unk|>\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "dbbYM5zk3gZL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello do you like tea?\"\n",
        "text2 = \"anything else you need while having tea \"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1,text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2JNpIKv6F_9",
        "outputId": "9e012c18-a182-49a1-cfd3-24f686ae7c3f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello do you like tea? <|endoftext|> anything else you need while having tea \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zhdCq3_6rLG",
        "outputId": "29b6ffcd-1185-4cfd-d31f-ede3269d18ae"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9056, 3597, 9031, 5399, 8030, 625, 9057, 2175, 3830, 9031, 5867, 8800, 4756, 8030]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kihyy8rR6uf2",
        "outputId": "05e781b9-940b-4688-dac1-210fd4ca9386"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> do you like tea? <|endoftext|> anything else you need while having tea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BYTE Pair Encoding (BPE)**"
      ],
      "metadata": {
        "id": "jJMn7dpQ_9tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXH7Z08gAHYJ",
        "outputId": "50a2a954-2ccd-40e1-cbb8-bad4f9336920"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import importlib\n",
        "\n",
        "print(\"Tiktoken Version : \" , importlib.metadata.version('tiktoken'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-o9QSFs48D3",
        "outputId": "f19060e0-e93a-4427-fa08-5e6a495ff3d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiktoken Version :  0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "Jw2Gix6L5az7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This encoder is similar to the **SimpleTokenizerV2**"
      ],
      "metadata": {
        "id": "XWciGlhJ5xw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, How Are you doing today? <|endoftext|> Where you will head nextfromhere\"\n",
        ")\n",
        "\n",
        "ids = tokenizer.encode(text , allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQdgO1jl5wvK",
        "outputId": "0c7c0be0-172e-4e9d-e255-27e87893047e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 1374, 4231, 345, 1804, 1909, 30, 220, 50256, 6350, 345, 481, 1182, 1306, 6738, 1456]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(ids)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwGpVJl37x0X",
        "outputId": "adf946a3-23b2-46ad-d969-2a48dbbbedec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, How Are you doing today? <|endoftext|> Where you will head nextfromhere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name= \"sahil\"\n",
        "integers = tokenizer.encode(f\"HEllo , boss {name} what -- are you doing here? \")\n",
        "print(integers)\n",
        "\n",
        "str = tokenizer.decode(integers)\n",
        "print(str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJaNzk1Q9R7T",
        "outputId": "8c9ad1f6-fefb-44de-c177-dc534d73f3d9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13909, 18798, 837, 6478, 473, 71, 346, 644, 1377, 389, 345, 1804, 994, 30, 220]\n",
            "HEllo , boss sahil what -- are you doing here? \n"
          ]
        }
      ]
    }
  ]
}